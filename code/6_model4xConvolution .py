# -*- coding: utf-8 -*-
"""ML_finPro_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v84mtrN9CBIDMWD3yFfJEQF1h_GlM-by
"""

import numpy as np
import pandas as pd

# from google.colab import drive
# drive.mount('/content/gdrive') # 此處需要登入google帳號
# 獲取授權碼之後輸入即可連動雲端硬碟   data = pd.read_csv("/content/gdrive/My Drive/已存在google雲端的檔名")

# aaa = pd.read_csv('/content/gdrive/My Drive/ML_Final/group_3.csv')
# print(aaa.head(5))
raw_file = pd.read_csv('../data/Fin_train.csv')
# raw_file = pd.read_csv('/content/gdrive/My Drive/ML_Final/Fin_train.csv')
print(raw_file.head(5))

from sklearn.model_selection import train_test_split
train_file, val_file = train_test_split(raw_file, random_state=777, train_size=0.8)

train_data=train_file.iloc[:,:-1]
train_label=train_file.iloc[:,-1:]
val_data=val_file.iloc[:,:-1]
val_label=val_file.iloc[:,-1:]

#測試集 前處理
print('x_train_image: ',train_data.shape)
print('y_train_image: ',train_label.shape)
train_data_re = train_data.values.reshape(8000,80,80)
print('train data re',train_data_re.shape)
train_label_re = train_label.values.reshape(8000,)
print('train label re',train_label_re.shape)
print('train label re data type is',train_label_re[0].dtype)
print('train_data_re[0]',train_data_re[0].shape)

#驗證集 前處理
print('val data: ',val_data.shape)
print('val label: ',val_label.shape)

val_data_re = val_data.values.reshape(2000,80,80)
print('val_data_re: ',val_data_re.shape)

val_label_re = val_label.values.reshape(2000,)
print('val_label_re',val_label_re.shape)

train_data4D = train_data_re.reshape(train_data_re.shape[0],80,80,1).astype('float32')
val_data4D = val_data_re.reshape(val_data_re.shape[0],80,80,1).astype('float32')
print('train_data4D',train_data4D.shape)
print('val_data4D',val_data4D.shape)

# 目標輸出
raw_data=raw_file.iloc[:,:-1]
raw_label=raw_file.iloc[:,-1:]
print('label:',raw_label.shape)
raw_label_re = raw_label.values.reshape(10000,)
print('label re:',raw_label_re.shape)
print(raw_data.head(5))
print(raw_data.shape)
raw_data_re = raw_data.values.reshape(10000,80,80)
print(raw_data_re.shape)
raw_data4D = raw_data_re.reshape(raw_data_re.shape[0],80,80,1).astype('float32')
print(raw_data4D.shape)

from matplotlib.pyplot import *
import matplotlib.pyplot as plt

from matplotlib.pyplot import *
plt.figure()
plt.imshow(train_data_re[0])
plt.colorbar()
plt.grid(False)
plt.show()

from keras.models import Sequential
from keras.layers import Dense,Dropout,Flatten,Conv2D,MaxPooling2D

model = Sequential()

model.add(Conv2D(
        filters=16,
        kernel_size=(5,5),
        padding='same',
        input_shape=(80,80,1),
        activation='relu'         ))

model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Conv2D(filters=32,
         kernel_size=(5,5),
         padding = 'same',
         activation='relu'))

model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Conv2D(filters=64,
         kernel_size=(5,5),
         padding = 'same',
         activation='relu'))

model.add(Conv2D(filters=128,
         kernel_size=(5,5),
         padding = 'same',
         activation='relu'))

model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1,activation='relu'))
model.compile(loss='mean_squared_error',optimizer='adam',metrics=['accuracy'])

model.save('initial.h5')

train_history=model.fit(x=train_data4D,
            y=train_label_re,validation_split=0.2,
            epochs=250,batch_size=500,verbose=2,
            validation_data=(val_data4D, val_label_re))

model.save('Treasure.h5')

import dill as pickle

with open('TreasureHistoryDict', 'wb') as handle:
        pickle.dump(train_history.history, handle)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
acc = train_history.history['accuracy']
val_acc = train_history.history['val_accuracy']
loss = train_history.history['loss']
val_loss = train_history.history['val_loss']

epochs = range(1, len(acc) + 1)

# "b" is for "blue line"
plt.plot(epochs, loss, 'b', label='Training loss')
# r is for "solid red line"
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
# plt.xlim((0, 250))
# plt.ylim((0, 10000))
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

from google.colab import drive
drive.mount('/content/drive')

print(model.summary())

raw_pred = model.predict(raw_data4D, batch_size=128)
print('raw_pred',raw_pred.shape)
raw_pred_re= raw_pred.reshape(10000,)
print(raw_pred_re)
raw_pred_list = list(raw_pred_re)
data_pred=raw_pred.reshape(10000)
print(data_pred)
print('raw_label_re shape',raw_label_re.shape)
print('raw_label_re',raw_label_re)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

plt.plot(data_pred,'b',label='Predict')
plt.plot(raw_label_re,'r')
plt.show()

# 評估預測誤差
def Mape(data_true, data_pred):
  return np.mean(np.abs((data_true - data_pred) / data_true )) * 100

Mape(raw_label_re,data_pred)

dataframe = pd.DataFrame({'ML':raw_pred_list})
#將DataFrame儲存為csv,index表示是否顯示行名，default=True
dataframe.to_csv("ML.csv",index=False,sep=',')
ML= pd.read_csv('ML.csv')
print(ML.head(5))
ML.shape

print('raw_file',raw_file.shape)
targetdata=np.column_stack((raw_file,ML))
print('targetdata.',targetdata.shape)

# from google.colab import drive
# drive.mount('/content/gdrive')

targetdata_pd = pd.DataFrame(data=targetdata)
# targetdata_pd.to_csv('Fin_target.csv',index=0)
# targetdata_pd.to_csv('/content/gdrive/My Drive/ML_Final/Fin_target.csv')
targetdata_pd.to_csv('../output/4xConvolution/group_3.csv')
print(targetdata_pd.head(5))

